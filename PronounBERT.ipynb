{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "#data libraries stuff\nimport numpy as np\nimport pandas as pd\nimport os\nprint(os.listdir(\"../input\"))\nimport zipfile\nimport sys\nimport time",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "#bert\n\"\"\"!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\nThis one ^^^^ is the smaller bert\"\"\"\n!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-24_H-1024_A-16.zip\nwith zipfile.ZipFile(\"uncased_L-24_H-1024_A-16.zip\",\"r\") as zip_ref:\n    zip_ref.extractall()\n!ls 'uncased_L-24_H-1024_A-16'\n!rm uncased_L-24_H-1024_A-16.zip\n\n#adabound is pretty cool\n!git clone https://github.com/titu1994/keras-adabound && mv keras-adabound/adabound.py ./ && rm -rf keras-adabound",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3e0ac6bb63d1487866640ebe8e73f78b3a96c25a"
      },
      "cell_type": "code",
      "source": "!wget https://raw.githubusercontent.com/google-research/bert/master/modeling.py \n!wget https://raw.githubusercontent.com/google-research/bert/master/extract_features.py \n!wget https://raw.githubusercontent.com/google-research/bert/master/tokenization.py\n!ls",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cfccbec6c87185a0db428e3ce8ecb93aa9c4547e"
      },
      "cell_type": "code",
      "source": "import modeling\nimport extract_features\nimport tokenization\nimport tensorflow as tf\nfrom adabound import AdaBound",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "64c3f40f620c50434a4645a76fe5ad1c9d34ec74"
      },
      "cell_type": "code",
      "source": "!wget https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-development.tsv\n!wget https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-validation.tsv\n!wget https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-test.tsv",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1a655a90d41802605da6f27c605313eac4af4cc2"
      },
      "cell_type": "code",
      "source": "def compute_offset_no_spaces(text, offset):\n\tcount = 0\n\tfor pos in range(offset):\n\t\tif text[pos] != \" \": count +=1\n\treturn count\n\ndef count_chars_no_special(text):\n\tcount = 0\n\tspecial_char_list = [\"#\"]\n\tfor pos in range(len(text)):\n\t\tif text[pos] not in special_char_list: count +=1\n\treturn count\n\ndef count_length_no_special(text):\n\tcount = 0\n\tspecial_char_list = [\"#\", \" \"]\n\tfor pos in range(len(text)):\n\t\tif text[pos] not in special_char_list: count +=1\n\treturn count",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9d8437cb4f7c1737e0c915d7d16cc3d004612416"
      },
      "cell_type": "code",
      "source": "def run_bert(data):\n\n\t#Needs to have emb_A, emb_B,emb_P, and the label connecting the actual pronoun to the worb\n\n    # From the current file, take the text only, and write it in a file which will be passed to BERT\n\ttext = data[\"Text\"]\n\ttext.to_csv(\"input.txt\", index = False, header = False)\n\n\tos.system(\"python3 extract_features.py \\\n\t  --input_file=input.txt \\\n\t  --output_file=output.jsonl \\\n\t  --vocab_file=uncased_L-24_H-1024_A-16/vocab.txt \\\n\t  --bert_config_file=uncased_L-24_H-1024_A-16/bert_config.json \\\n\t  --init_checkpoint=uncased_L-24_H-1024_A-16/bert_model.ckpt \\\n\t  --layers=-1 \\\n\t  --max_seq_length=256 \\\n\t  --batch_size=8\")\n\n\tbert_output = pd.read_json(\"output.jsonl\", lines = True)\n\n\tos.system(\"rm output.jsonl\")\n\tos.system(\"rm input.txt\")\n\n\tindex = data.index\n\tcolumns = [\"emb_A\", \"emb_B\", \"emb_P\", \"label\"]\n\temb = pd.DataFrame(index = index, columns = columns)\n\temb.index.name = \"ID\"\n\n\"\"\"\n\nThis is lowercase for uncased bert\n\tfor i in range(len(data)): # For each line in the data file\n\t\tP = data.loc[i,\"Pronoun\"].lower()\n\t\tA = data.loc[i,\"A\"].lower()\n\t\tB = data.loc[i,\"B\"].lower()\n\n\"\"\"\n\n#This bert uses capitals\n\tfor i in range(len(data)): # For each line in the data file\n\t\tP = data.loc[i,\"Pronoun\"]\n\t\tA = data.loc[i,\"A\"]\n\t\tB = data.loc[i,\"B\"]\n        \n        \n\n\t\tP_offset = compute_offset_no_spaces(data.loc[i,\"Text\"], data.loc[i,\"Pronoun-offset\"])\n\t\tA_offset = compute_offset_no_spaces(data.loc[i,\"Text\"], data.loc[i,\"A-offset\"])\n\t\tB_offset = compute_offset_no_spaces(data.loc[i,\"Text\"], data.loc[i,\"B-offset\"])\n\t\tA_length = count_length_no_special(A)\n\t\tB_length = count_length_no_special(B)\n\n\t\t# init embeddings\n\t\temb_A = np.zeros(1024)\n\t\temb_B = np.zeros(1024)\n\t\temb_P = np.zeros(1024)\n\n\t\t# init counts\n\t\tcount_chars = 0\n\t\tcnt_A, cnt_B, cnt_P = 0, 0, 0\n\n\t\tfeatures = pd.DataFrame(bert_output.loc[i,\"features\"])\n\t\tfor j in range(2,len(features)): \n\t\t\ttoken = features.loc[j,\"token\"]\n\n\n\t\t\tif count_chars  == P_offset: \n\t\t\t\temb_P += np.array(features.loc[j,\"layers\"][0]['values'])\n\t\t\t\tcnt_P += 1\n\t\t\tif count_chars in range(A_offset, A_offset + A_length): \n\t\t\t\temb_A += np.array(features.loc[j,\"layers\"][0]['values'])\n\t\t\t\tcnt_A +=1\n\t\t\tif count_chars in range(B_offset, B_offset + B_length): \n\t\t\t\temb_B += np.array(features.loc[j,\"layers\"][0]['values'])\n\t\t\t\tcnt_B +=1\t\t\t\t\t\t\t\t\n\t\t\tcount_chars += count_length_no_special(token)\n\t\temb_A /= cnt_A\n\t\temb_B /= cnt_B\n\n\n\t\tlabel = \"Neither\"\n\t\tif (data.loc[i,\"A-coref\"] == True):\n\t\t\tlabel = \"A\"\n\t\tif (data.loc[i,\"B-coref\"] == True):\n\t\t\tlabel = \"B\"\n\n\n\t\temb.iloc[i] = [emb_A, emb_B, emb_P, label]\n\n\treturn emb",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8298947fa33285e722bdaae2df41bca5dd795732"
      },
      "cell_type": "code",
      "source": "print(\"Started at \", time.ctime())\ntest_data = pd.read_csv(\"gap-test.tsv\", sep = '\\t')\ntest_emb = run_bert(test_data)\ntest_emb.to_json(\"contextual_embeddings_gap_test.json\", orient = 'columns')\n\nvalidation_data = pd.read_csv(\"gap-validation.tsv\", sep = '\\t')\nvalidation_emb = run_bert(validation_data)\nvalidation_emb.to_json(\"contextual_embeddings_gap_validation.json\", orient = 'columns')\n\ndevelopment_data = pd.read_csv(\"gap-development.tsv\", sep = '\\t')\ndevelopment_emb = run_bert(development_data)\ndevelopment_emb.to_json(\"contextual_embeddings_gap_development.json\", orient = 'columns')\nprint(\"Finished at \", time.ctime())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "29ba41d2570238ec22735909c76cd86c2742517c"
      },
      "cell_type": "code",
      "source": "from keras import backend, models, layers, initializers, regularizers, constraints, optimizers\nfrom keras import callbacks as kc\nfrom keras import optimizers as ko\n\nfrom sklearn.model_selection import cross_val_score, KFold, train_test_split\nfrom sklearn.metrics import log_loss\nimport time\n\n\ndense_layer_sizes = [64,128,64]\ndropout_rate = 0.4\nlearning_rate = 0.001\nn_fold = 5\nbatch_size = 64\nepochs = 1000\npatience = 100\nlambd = 0.1 ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "763ec8591474c45d6b065cad0c7efc2bbe9ad514"
      },
      "cell_type": "code",
      "source": "def build_mlp_model(input_shape):\n\tX_input = layers.Input(input_shape)\n\n\tX = layers.Dense(dense_layer_sizes[0], name = 'dense0')(X_input)\n\tX = layers.BatchNormalization(name = 'bn0')(X)\n\tX = layers.Activation('relu')(X)\n\tX = layers.Dropout(dropout_rate)(X)\n\n\tX = layers.Dense(dense_layer_sizes[1], name = 'dense1')(X)\n\tX = layers.BatchNormalization(name = 'bn1')(X)\n\tX = layers.Activation('relu')(X)\n\tX = layers.Dropout(dropout_rate)(X)\n\n\tX = layers.Dense(3, name = 'output', kernel_regularizer = regularizers.l2(lambd))(X)\n\tX = layers.Activation('softmax')(X)\n\n\tmodel = models.Model(input = X_input, output = X, name = \"classif_model\")\n\treturn model",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0ea0ac2603b0a4bbdaa2776c8e37ad7894a99f5a"
      },
      "cell_type": "code",
      "source": "def parse_json(embeddings):\n\n\tembeddings.sort_index(inplace = True) # Sorting the DataFrame, because reading from the json file messed with the order\n\tX = np.zeros((len(embeddings),3*1024))\n\tY = np.zeros((len(embeddings), 3))\n\n\n\tfor i in range(len(embeddings)):\n\t\tA = np.array(embeddings.loc[i,\"emb_A\"])\n\t\tB = np.array(embeddings.loc[i,\"emb_B\"])\n\t\tP = np.array(embeddings.loc[i,\"emb_P\"])\n\t\tX[i] = np.concatenate((A,B,P))\n\n\n\tfor i in range(len(embeddings)):\n\t\tlabel = embeddings.loc[i,\"label\"]\n\t\tif label == \"A\":\n\t\t\tY[i,0] = 1\n\t\telif label == \"B\":\n\t\t\tY[i,1] = 1\n\t\telse:\n\t\t\tY[i,2] = 1\n\n\treturn X, Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "870abe398fa7a8cd20aa42953bcf592b0db5469b"
      },
      "cell_type": "code",
      "source": "development = pd.read_json(\"contextual_embeddings_gap_development.json\")\nX_development, Y_development = parse_json(development)\n\nvalidation = pd.read_json(\"contextual_embeddings_gap_validation.json\")\nX_validation, Y_validation = parse_json(validation)\n\ntest = pd.read_json(\"contextual_embeddings_gap_test.json\")\nX_test, Y_test = parse_json(test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3b942d4712312b1c03b657b83aafd6d6595179c4"
      },
      "cell_type": "code",
      "source": "#drop rws\nremove_test = [row for row in range(len(X_test)) if np.sum(np.isnan(X_test[row]))]\nX_test = np.delete(X_test, remove_test, 0)\nY_test = np.delete(Y_test, remove_test, 0)\n\nremove_validation = [row for row in range(len(X_validation)) if np.sum(np.isnan(X_validation[row]))]\nX_validation = np.delete(X_validation, remove_validation, 0)\nY_validation = np.delete(Y_validation, remove_validation, 0)\n\n#not for test\nremove_development = [row for row in range(len(X_development)) if np.sum(np.isnan(X_development[row]))]\nX_development[remove_development] = np.zeros(3*768)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "25de1c995bf67e68ffd342063eb5a363c43fe459"
      },
      "cell_type": "code",
      "source": "X_train = np.concatenate((X_test, X_validation), axis = 0)\nY_train = np.concatenate((Y_test, Y_validation), axis = 0)\n\n\nprediction = np.zeros((len(X_development),3)) # testing predictions",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2538dcc94af4b17eb2c34112f62cf5244d2329ca",
        "scrolled": false
      },
      "cell_type": "code",
      "source": "folds = KFold(n_splits=n_fold, shuffle=True, random_state=3)\nscores = []\nfor fold_n, (train_index, valid_index) in enumerate(folds.split(X_train)):\n\tprint('Fold', fold_n, 'started at', time.ctime())\n\tX_tr, X_val = X_train[train_index], X_train[valid_index]\n\tY_tr, Y_val = Y_train[train_index], Y_train[valid_index]\n\n\tclassif_model = build_mlp_model([X_train.shape[1]])\n\tclassif_model.compile(optimizer = optimizers.Nadam(lr = learning_rate), loss = \"categorical_crossentropy\")\n\tcallbacks = [kc.EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights = True)]\n\n\tclassif_model.fit(x = X_tr, y = Y_tr, epochs = epochs, batch_size = batch_size, callbacks = callbacks, validation_data = (X_val, Y_val), verbose = 0)\n\n\tpred_valid = classif_model.predict(x = X_val, verbose = 0)\n\tpred = classif_model.predict(x = X_development, verbose = 0)\n\n\tscores.append(log_loss(Y_val, pred_valid))\n\tprint(log_loss(Y_val, pred_valid))\n\tprediction += pred\nprediction /= n_fold\n\n\nprint('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\nprint(scores)\n#dont see test score until end",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "372b9720d8d6719332ebf4ae14b64187292dede2"
      },
      "cell_type": "code",
      "source": "#print(\"Test score:\", log_loss(Y_development,prediction))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "198c1b2df108afcf2292538c1b12b10c05d9c12b"
      },
      "cell_type": "code",
      "source": "submission = pd.read_csv(\"../input/sample_submission_stage_1.csv\", index_col = \"ID\")\nsubmission[\"A\"] = prediction[:,0]\nsubmission[\"B\"] = prediction[:,1]\nsubmission[\"NEITHER\"] = prediction[:,2]\nsubmission.to_csv(\"submission_bert.csv\")",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}
